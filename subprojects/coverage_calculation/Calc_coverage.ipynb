{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysam\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking how the coverage is calculated\n",
    "\n",
    "Attempt to determine details of the coverage calculation. We use the Picard tool CollectWgsMetrics to calculate the coverage.\n",
    "\n",
    "The experiment consists of two components: `script.sh` and this notebook.\n",
    "\n",
    "**The experiment was a failure, and an alternative method -- reading the source code -- was used to arrive at a conclusion.**\n",
    "\n",
    "Method:\n",
    "* Extract a few reads from the FASTQ files and align them to the reference, to have a small SAM/BAM file to work with.\n",
    "* Compute the genomic coverage (for information)\n",
    "* Mark duplicates in the SAM file.\n",
    "* Compute the coverage again as a baseline.\n",
    "* Remove some records from the SAM file. The same set of records is analysed in this notebook.\n",
    "* Compute the coverage of the new SAM file. Compare the difference with the metrics in this file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From the Picard documentation\n",
    "\n",
    "https://gatk.broadinstitute.org/hc/en-us/articles/360036452852-CollectWgsMetrics-Picard-\n",
    "\n",
    "The following options (defaults listed here) affect the computed coverage:\n",
    "* `COUNT_UNPAIRED` False\n",
    "* `MINIMUM_BASE_QUALITY`: 20\n",
    "* `MINIMUM_MAPPING_QUALITY`: 20\n",
    "\n",
    "Other options aren't relevant for the calculation of overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = pysam.AlignmentFile(\"test/sample.markdup.bam\", \"rb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coverage calculation requires a big RAM to hold coverage for each position\n",
    "#f.count_coverage(\"chr1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First read in the sorted file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E00426:63:HYK7TCCXY:5:1104:24850:7304 ..skipped\n",
      "E00426:63:HYK7TCCXY:5:1104:24850:7304 ..skipped\n",
      "E00426:63:HYK7TCCXY:5:1104:20456:7462 ..skipped\n",
      "E00426:63:HYK7TCCXY:5:1104:20456:7462 ..skipped\n",
      "\n",
      " E00426:63:HYK7TCCXY:5:1104:24495:7708\n",
      "> chr1:623652\n",
      "> R1: False , is_reverse: False , is_proper_pair: True , is_secondary: False , is_duplicate: False\n",
      "> mapping_quality: 24\n",
      "\n",
      " E00426:63:HYK7TCCXY:5:1104:24495:7708\n",
      "> chr1:624199\n",
      "> R1: True , is_reverse: True , is_proper_pair: True , is_secondary: False , is_duplicate: False\n",
      "> mapping_quality: 24\n",
      "E00426:63:HYK7TCCXY:5:1104:20791:7058 ..skipped\n",
      "E00426:63:HYK7TCCXY:5:1104:30076:7603 ..skipped\n",
      "E00426:63:HYK7TCCXY:5:1104:30076:7603 ..skipped\n",
      "\n",
      " E00426:63:HYK7TCCXY:5:1104:17797:7110\n",
      "> chr1:738788\n",
      "> R1: False , is_reverse: False , is_proper_pair: True , is_secondary: False , is_duplicate: False\n",
      "> mapping_quality: 44\n",
      "\n",
      " E00426:63:HYK7TCCXY:5:1104:17797:7110\n",
      "> chr1:738948\n",
      "> R1: True , is_reverse: True , is_proper_pair: True , is_secondary: False , is_duplicate: False\n",
      "> mapping_quality: 40\n",
      "\n",
      " E00426:63:HYK7TCCXY:5:1104:28544:8112\n",
      "> chr1:791554\n",
      "> R1: False , is_reverse: True , is_proper_pair: True , is_secondary: False , is_duplicate: False\n",
      "> mapping_quality: 22\n",
      "\n",
      " E00426:63:HYK7TCCXY:5:1104:28544:8112\n",
      "> chr1:791575\n",
      "> R1: True , is_reverse: False , is_proper_pair: True , is_secondary: False , is_duplicate: False\n",
      "> mapping_quality: 22\n",
      "\n",
      " E00426:63:HYK7TCCXY:5:1104:8014:8341\n",
      "> chr1:796982\n",
      "> R1: False , is_reverse: False , is_proper_pair: True , is_secondary: False , is_duplicate: False\n",
      "> mapping_quality: 48\n",
      "\n",
      " E00426:63:HYK7TCCXY:5:1104:8014:8341\n",
      "> chr1:796986\n",
      "> R1: True , is_reverse: True , is_proper_pair: True , is_secondary: False , is_duplicate: False\n",
      "> mapping_quality: 47\n",
      "\n",
      "\n",
      "Read 14 records from the file. Found 8 alignments satisfying the criteria.\n"
     ]
    }
   ],
   "source": [
    "reads = []\n",
    "want = 8\n",
    "for count, r in enumerate(f.fetch()):\n",
    "    if r.mapping_quality < 20 or not r.is_proper_pair:\n",
    "        print(r.query_name, \"..skipped\")\n",
    "        continue\n",
    "    else:\n",
    "        print(\"\\n\", r.query_name)\n",
    "    print(\">\", r.reference_name + \":\" + str(r.pos))\n",
    "    print(\"> R1:\", r.is_read1, \", is_reverse:\", r.is_reverse, \", is_proper_pair:\", r.is_proper_pair,\n",
    "         \", is_secondary:\", r.is_secondary, \", is_duplicate:\", r.is_duplicate)\n",
    "    print(\"> mapping_quality:\", r.mapping_quality)\n",
    "    \n",
    "    reads.append(r)\n",
    "    if len(reads) == want:\n",
    "        break\n",
    "\n",
    "print(\"\\n\\nRead\", count, \"records from the file. Found\", len(reads), \"alignments satisfying the criteria.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alignments are selected for further analysis in the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse read pairs\n",
    "\n",
    "Compute the overlap, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- pair 1 -\n",
      "Insert size: 698\n",
      "Combined aligned ref length: 302 ( 151 + 151 )\n",
      "Overlap: -396\n",
      "Covered at Q>=20: 255 ( 108 + 147 )\n",
      "\n",
      "- pair 2 -\n",
      "Insert size: 311\n",
      "Combined aligned ref length: 302 ( 151 + 151 )\n",
      "Overlap: -9\n",
      "Covered at Q>=20: 283 ( 139 + 144 )\n",
      "\n",
      "- pair 3 -\n",
      "Insert size: 92\n",
      "Combined aligned ref length: 135 ( 64 + 71 )\n",
      "Overlap: 43\n",
      "Covered at Q>=20: 123 ( 53 + 70 )\n",
      "\n",
      "- pair 4 -\n",
      "Insert size: 155\n",
      "Combined aligned ref length: 302 ( 151 + 151 )\n",
      "Overlap: 147\n",
      "Covered at Q>=20: 243 ( 120 + 123 )\n",
      "\n",
      "\n",
      "total bases:            1041\n",
      "uniquely covered bases: 851\n",
      "bases at Q >= 20:       904\n"
     ]
    }
   ],
   "source": [
    "read_pairs = zip(reads[::2], reads[1::2])\n",
    "reads_bases = 0\n",
    "reads_bases_no_overlap = 0\n",
    "reads_q20_bases = 0\n",
    "\n",
    "for i,(ra, rb) in enumerate(read_pairs):\n",
    "    print(\"- pair\", i+1, \"-\")\n",
    "    insert_size = (rb.pos + rb.reference_length) - (ra.pos)\n",
    "    print(\"Insert size:\", insert_size)\n",
    "    read_bases = ra.reference_length + rb.reference_length\n",
    "    print(\"Combined aligned ref length:\", read_bases, \"(\", ra.reference_length, \"+\", rb.reference_length,\")\")\n",
    "    print(\"Overlap:\", read_bases - insert_size)\n",
    "    reads_bases += read_bases\n",
    "    reads_bases_no_overlap += min(insert_size, read_bases)\n",
    "    q20_bases_a = sum(q >= 20 for q in ra.query_alignment_qualities)\n",
    "    q20_bases_b = sum(q >= 20 for q in rb.query_alignment_qualities)\n",
    "    print(\"Covered at Q>=20:\", q20_bases_a + q20_bases_b, \"(\", q20_bases_a, \"+\", q20_bases_b, \")\")\n",
    "    reads_q20_bases += q20_bases_a + q20_bases_b\n",
    "    print(\"\")\n",
    "\n",
    "\n",
    "print(\"\")\n",
    "print(\"total bases:           \", reads_bases)\n",
    "print(\"uniquely covered bases:\", reads_bases_no_overlap)\n",
    "print(\"bases at Q >= 20:      \", reads_q20_bases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of the last two readÂ pairs\n",
    "\n",
    "Both reads in pair 3 are soft-clipped in both ends (before and after match to reference)! The alignment is probably spurious / bogus. The actual insert size reported in IGV is 43, taking into account the soft clipping. \n",
    "\n",
    "The number of bases with Q=>20 -- 123 -- is too high to allow the possibility that doubly covered bases are only counted once / given a quality score of 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uniquely covered bases: 92\n"
     ]
    }
   ],
   "source": [
    "overlap = 43\n",
    "ra_once_covered = 64 - overlap\n",
    "rb_once_covered = 71 - overlap\n",
    "print(\"Uniquely covered bases:\", ra_once_covered + rb_once_covered + overlap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fourth pair has a simpler configuration without soft-clipping. Almost all of the bases are overlapping because the insert size is only 155. The base quality >=20 is clearly not compatible with having overlapping bases set to zeros. The max possible should have been:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected maximum if only count covered bases once: 155\n"
     ]
    }
   ],
   "source": [
    "ra_ref_length, rb_ref_length = 151, 151\n",
    "overlap = 147\n",
    "print(\"expected maximum if only count covered bases once:\",ra_ref_length + rb_ref_length - overlap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is, the same as the insert size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation of the coverage using Picard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_picard_coverage(path):\n",
    "    with open(path) as f:\n",
    "        flag = False\n",
    "        for l in f:\n",
    "            parts = l.split('\\t')\n",
    "            if flag:\n",
    "                return [float(p) for p in parts[0:2]]\n",
    "            if parts[0:2] == ['GENOME_TERRITORY','MEAN_COVERAGE']:\n",
    "                flag = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage before mark duplicates: 0.001443\n",
      "Coverage after mark duplicates: 0.001384\n",
      "Coverage after removing the reads: 0.001384\n"
     ]
    }
   ],
   "source": [
    "territory, coverage_0 = get_picard_coverage('test/sample_wgs_metrics.txt')\n",
    "print(\"Coverage before mark duplicates:\", coverage_0)\n",
    "_, coverage_full = get_picard_coverage('test/sample_wgs_metrics_post_markdup.txt')\n",
    "print(\"Coverage after mark duplicates:\", coverage_full)\n",
    "_, coverage_tail = get_picard_coverage('test/tail_wgs_metrics.txt')\n",
    "print(\"Coverage after removing the reads:\", coverage_tail)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "The output from Picard is not precise enough to make this analysis easy. The coverage has the \"genome territory\" -- the size of the genome -- in the denominator, and thus the difference in coverage caused by a few reads is a very small number.\n",
    "\n",
    "Excluding more reads in the second run would be an option, but the next paragraph shows that it's not really necessary.\n",
    "\n",
    "\n",
    "## Picard source code gives the answer\n",
    "\n",
    "The CollectWgsMetrics source has a function `addInfo` that is called at every locus in the reference. It includes an if statement that ensures that only one read of each query name is counted. Read pairs have the same query name, and all other reads have different query names, so this ensures that overlap regions are only counted once.\n",
    "\n",
    "https://github.com/broadinstitute/picard/blob/master/src/main/java/picard/analysis/CollectWgsMetrics.java#L425\n",
    "\n",
    "The tool provides an output column `PCT_EXC_OVERLAP` that informs us of the fraction of bases excluded due to overlap.\n",
    "\n",
    "## Summary of conditions for counting coverage\n",
    "\n",
    "The following bases are excluded when computing the coverage:\n",
    "\n",
    "* Adapter sequences\n",
    "* All bases in reads with mapping quality < 20 (including unmapped reads)\n",
    "* Duplicates\n",
    "* Unpaired reads\n",
    "* Secondary alignments\n",
    "* With base quality < 20\n",
    "* Overlap\n",
    "\n",
    "There are also caps for max coverage to count at each given locus.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
