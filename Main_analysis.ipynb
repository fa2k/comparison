{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import re\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which downsampling norm to use: This selects input data:\n",
    "# dd: Deduplicated downsampled data\n",
    "# aa: Analyse all reads in data, downsample based on number of PF reads\n",
    "DS_MODE = \"aa\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alignment notes\n",
    "\n",
    "Alignment with bwa-mem.\n",
    "\n",
    "The minimum seed length is 19.\n",
    "\n",
    "Maximum gap length: 100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Picard data\n",
    "\n",
    "### Step 1 is to load all the metrics from the downsampled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_picard(path):\n",
    "    with open(path) as f:\n",
    "        mode = None\n",
    "        metrics_header = None\n",
    "        metrics_data = {}\n",
    "        histogram_series = []\n",
    "        histogram_data = []\n",
    "        for l in f:\n",
    "            line = l.rstrip(\"\\r\\n\")\n",
    "            if mode == 'metrics':\n",
    "                metrics_header = line.split()\n",
    "                mode = 'metrics2'\n",
    "            elif mode == 'metrics2':\n",
    "                if line != \"\":\n",
    "                    metrics_data = dict(zip(metrics_header, line.split()))\n",
    "                else:\n",
    "                    mode = None\n",
    "            elif mode == 'histogram':\n",
    "                histogram_series = line.split()\n",
    "                histogram_data = [list() for _ in histogram_series]\n",
    "                mode = 'histogram2'\n",
    "            elif mode == 'histogram2':\n",
    "                if line != \"\":\n",
    "                    for i, value in enumerate(line.split()):\n",
    "                        histogram_data[i].append(value)\n",
    "                else:\n",
    "                    mode = None\n",
    "            elif line.startswith(\"## METRICS CLASS\"):\n",
    "                mode = 'metrics'\n",
    "            elif line.startswith(\"## HISTOGRAM\"):\n",
    "                mode = 'histogram'\n",
    "        if histogram_series:\n",
    "            #metrics_data['histograms'] = dict(zip(histogram_series, histogram_data))\n",
    "            for name, data in zip(histogram_series, histogram_data):\n",
    "                metrics_data[name] = [float(x) for x in data]\n",
    "    return metrics_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = []\n",
    "for bam in glob(\"30_downsample/*-{}_DS_MD.AlignmentSummaryMetrics.txt\".format(DS_MODE)):\n",
    "    basepath = re.sub(r\"_MD\\.AlignmentSummaryMetrics\\.txt$\", \"\", bam)\n",
    "    data = {}\n",
    "    try:\n",
    "        for metrics in ['_MD.AlignmentSummaryMetrics', '_MD.InsertSizeMetrics',\n",
    "                        '.MarkDuplicatesMetrics', '_MD.WgsMetrics']:\n",
    "            new_data = load_picard(\"{}{}.txt\".format(basepath, metrics))\n",
    "            if any(k in data for k in new_data):\n",
    "                print(\"Duplicate key {} found in {}\".format(k, metrics))\n",
    "            data.update(new_data)\n",
    "    except FileNotFoundError as e:\n",
    "        print(\"File {} not found, skipping this sample.\".format(e.filename))\n",
    "        continue\n",
    "    samples.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n"
     ]
    }
   ],
   "source": [
    "print(len(samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Get original number of PF reads from before downsampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inital_read_counts = []\n",
    "for alignment_txt in glob(\"20_piccard/*.AlignmentSummaryMetrics.txt\"):\n",
    "    library = re.match(r\"([^/]+)\\.AlignmentSummaryMetrics.txt\", os.path.basename(alignment_txt)).group(1)\n",
    "    new_data = load_picard(alignment_txt)\n",
    "    inital_read_counts.append({'LIBRARY': library, 'INITIAL_READS': int(new_data['TOTAL_READS']),\n",
    "                                    'INITIAL_READS_PER_END': int(new_data['TOTAL_READS']) / 2,\n",
    "                                    'INITIAL_READS_ALIGNED': int(new_data['PF_READS_ALIGNED']),\n",
    "                                    'INITIAL_READS_ALIGNED_PER_END': int(new_data['PF_READS_ALIGNED']) / 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dedup_read_counts = []\n",
    "for rc_txt in glob(\"20_piccard_dd/*.readCount.txt\"):\n",
    "    library = re.match(r\"([^/]+)\\.readCount\\.txt\", os.path.basename(rc_txt)).group(1)\n",
    "    rc = int(open(rc_txt).read().strip())\n",
    "    dedup_read_counts.append({'LIBRARY': library, 'DEDUPLICATED_READS': rc, 'DEDUPLICATED_READS_PER_END': rc/2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Get arabidopsis metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "arabidopsis_samples = []\n",
    "for bam in glob(\"arabidopsis/20_piccard/*.bam\"):\n",
    "    basepath = re.sub(r\"\\.bam$\", \"\", bam)\n",
    "    data = {}\n",
    "    for metrics in ['.AlignmentSummaryMetrics', '.InsertSizeMetrics',\n",
    "                    '.MarkDuplicatesMetrics', '.WgsMetrics']:\n",
    "        new_data = load_picard(\"{}{}.txt\".format(basepath, metrics))\n",
    "        if any(k in data for k in new_data):\n",
    "            print(\"Duplicate key {} found in {}\".format(k, metrics))\n",
    "        data.update(new_data)\n",
    "    arabidopsis_samples.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nextera-10ng-1\n",
      "TODO. There is no arabidopsis yet.\n"
     ]
    }
   ],
   "source": [
    "print(samples[0]['LIBRARY'])\n",
    "#print(inital_read_counts[0]['LIBRARY'])\n",
    "print(\"TODO. There is no arabidopsis yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'LIBRARY'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-e0f251c316b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mmain_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0marabidopsis_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             on='LIBRARY', suffixes=('', '_AR')).reindex(), \n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0min_reads_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         on='LIBRARY').reindex(), \n",
      "\u001b[0;32m/ypool/applications/anaconda3/lib/python3.7/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m     45\u001b[0m                          \u001b[0mright_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mright_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuffixes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msuffixes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                          \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindicator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindicator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m                          validate=validate)\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ypool/applications/anaconda3/lib/python3.7/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    527\u001b[0m         (self.left_join_keys,\n\u001b[1;32m    528\u001b[0m          \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m          self.join_names) = self._get_merge_keys()\n\u001b[0m\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;31m# validate the merge keys dtypes. We may need to coerce\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ypool/applications/anaconda3/lib/python3.7/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m_get_merge_keys\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    831\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mrk\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m                             right_keys.append(\n\u001b[0;32m--> 833\u001b[0;31m                                 right._get_label_or_level_values(rk))\n\u001b[0m\u001b[1;32m    834\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m                             \u001b[0;31m# work-around for merge_asof(right_index=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ypool/applications/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_label_or_level_values\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1704\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1705\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1706\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1708\u001b[0m         \u001b[0;31m# Check for duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'LIBRARY'"
     ]
    }
   ],
   "source": [
    "main_df = pd.DataFrame(samples).reindex().apply(pd.to_numeric, axis=0, errors='ignore')\n",
    "in_reads_df = pd.DataFrame(inital_read_counts)\n",
    "dd_reads_df = pd.DataFrame(dedup_read_counts)\n",
    "arabidopsis_df = pd.DataFrame(arabidopsis_samples).apply(pd.to_numeric, axis=0, errors='ignore')\n",
    "\n",
    "\n",
    "# Without Arabidopsis\n",
    "#df = pd.merge( \n",
    "#    pd.merge(\n",
    "#        main_df, \n",
    "#        in_reads_df,  \n",
    "#        on='LIBRARY').reindex(), \n",
    "#    dd_reads_df,  \n",
    "#    on='LIBRARY')\n",
    "\n",
    "# Without Arabidopsis\n",
    "df = pd.merge( \n",
    "    pd.merge( \n",
    "        pd.merge( \n",
    "            main_df, \n",
    "            arabidopsis_df, \n",
    "            on='LIBRARY', suffixes=('', '_AR')).reindex(), \n",
    "        in_reads_df,  \n",
    "        on='LIBRARY').reindex(), \n",
    "    dd_reads_df,  \n",
    "    on='LIBRARY')\n",
    "df.dtypes[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['Kit'] = pd.Categorical(df.LIBRARY.str.split(\"-\").str.get(0))\n",
    "df['Conc'] = pd.Categorical(df.LIBRARY.str.split(\"-\").str.get(1))\n",
    "df_100 = df.loc[df.Conc == \"100ng\"]\n",
    "df.head(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of entries: \", len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of reads\n",
    "\n",
    "Number of PE reads (reads per end):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['TOTAL_READS_PER_END'] = df['TOTAL_READS'] / 2\n",
    "num_reads_downsampled = df.TOTAL_READS_PER_END.mean()\n",
    "num_reads_initial = df.INITIAL_READS_PER_END.mean()\n",
    "num_reads_deduplicated = df.DEDUPLICATED_READS_PER_END.mean()\n",
    "print(\"One lane of HiSeqX:       {:9d} (optimistic)\".format(400000000))\n",
    "print(\"Initial reads (avg):      {:9d}\".format(int(num_reads_initial)))\n",
    "print(\"Deduplicated reads (avg): {:9d}\".format(int(num_reads_deduplicated)))\n",
    "print(\"Downsampled reads:        {:9d}\".format(int(num_reads_downsampled)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment set-up:\n",
    "\n",
    "One pool for each kit. There are 8 libraries in each pool. The pool is sequenced on four lanes. Each library gets a half lane's worth of reads on average.\n",
    "\n",
    "The mean read count per library is around half of the expected 200M, because of less than optimal yield, and inevitable differences between samples in the pool.\n",
    "\n",
    "The \"downsampled\" in the table above depends on the constant set at the top of this notebook. It is either based on the lowest Initial reads (aa) or the lowest Deduplicated reads (dd)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(DS_MODE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downsampling is based on the following read counts: aa=INITIAL, dd=DEDUPLICATED.\n",
    "\n",
    "The point with the fewest reads represents the value used for downsampling (after rounding down)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.boxplot(data=df, x='Kit', y='INITIAL_READS_PER_END', hue='Conc')\n",
    "plt.ylabel('Number of reads per replicate')\n",
    "plt.xlabel(\"\")\n",
    "plt.savefig(\"plots/total_number_of_reads.pdf\")\n",
    "plt.show()\n",
    "sns.boxplot(data=df, x='Kit', y='DEDUPLICATED_READS_PER_END', hue='Conc')\n",
    "plt.title('Deduplicated reads')\n",
    "plt.show()\n",
    "sns.boxplot(data=df, x='Kit', y='TOTAL_READS_PER_END', hue='Conc')\n",
    "plt.title('Downsampled reads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df.Kit=='Swiftlong') & (df.Conc == '100ng')].PF_READS/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aligned reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([col for col in list(df) if 'ALIGNED' in col.upper()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot of any aligned reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=df, x='Kit', y='PCT_PF_READS_ALIGNED', hue='Conc')\n",
    "plt.title('Reads aligned to human genome')\n",
    "plt.ylabel(\"Fraction of reads\")\n",
    "plt.xlabel(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots for MAPQ > 20 reads\n",
    "\n",
    "Requiring MAPQ > 20 removes a large percentage of reads. We found a lot of reads that map to both references."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['PCT_HQ_PF_READS_ALIGNED'] = df.PF_HQ_ALIGNED_READS / df.TOTAL_READS\n",
    "sns.boxplot(data=df, x='Kit', y='PCT_HQ_PF_READS_ALIGNED', hue='Conc')\n",
    "plt.title('Reads aligned to human genome (MAPQ>=20)')\n",
    "plt.ylabel(\"Fraction of reads\")\n",
    "plt.xlabel(\"\")\n",
    "plt.savefig(\"plots/aligned_reads_human.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Low aligned rate\")\n",
    "df[df.PCT_HQ_PF_READS_ALIGNED < 0.88][[\"LIBRARY\", \"PCT_HQ_PF_READS_ALIGNED\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct the aligned pct. All reads in the AR dataset are aligned.\n",
    "df['PCT_HQ_PF_READS_ALIGNED_AR'] = df.PF_HQ_ALIGNED_READS_AR / df.INITIAL_READS\n",
    "sns.boxplot(data=df, x='Kit', y='PCT_HQ_PF_READS_ALIGNED_AR', hue='Conc')\n",
    "plt.title('Reads aligned to Arabidopsis Lyrata genome (MAPQ >= 20)')\n",
    "plt.ylabel(\"Fraction of reads\")\n",
    "plt.xlabel(\"\")\n",
    "plt.savefig(\"plots/aligned_reads_arabidopsis.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show arabidopsis in same plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = sns.boxplot(data=df, x='Kit', y='PCT_HQ_PF_READS_ALIGNED', hue='Conc', linewidth=0.5, fliersize=3)\n",
    "grid.legend().remove()\n",
    "sns.boxplot(data=df, x='Kit', y='PCT_HQ_PF_READS_ALIGNED_AR', hue='Conc', linewidth=0.5, fliersize=3)\n",
    "plt.ylim(0,1)\n",
    "plt.savefig(\"plots/aligned_reads_both.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "legend = grid.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative way to do combined plot here -- not used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['PCT_HQ_PF_READS_ALIGNED_AR'] = df.PF_HQ_ALIGNED_READS_AR / df.INITIAL_READS\n",
    "#df['PCT_HQ_PF_READS_ALIGNED'] = df.PF_HQ_ALIGNED_READS / df.TOTAL_READS\n",
    "hdf = pd.DataFrame(df)\n",
    "hdf['Value'] = hdf.PCT_HQ_PF_READS_ALIGNED\n",
    "hdf['Species'] = \"Human\"\n",
    "adf = pd.DataFrame(df)\n",
    "adf['Value'] = adf.PCT_HQ_PF_READS_ALIGNED_AR\n",
    "adf['Species'] = \"Arabidopsis\"\n",
    "combined = pd.concat([adf, hdf])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show details for one sample. This is to determine why there is so many reads aligned to both arabidopsis and human at the same time. This was never completely solved, but we see that the problem goes away when requiring a moderate mapping quality (20)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = df[df['LIBRARY'] == 'NEB-10ng-1'].iloc[0]\n",
    "print(\"sample:\", sample.LIBRARY)\n",
    "print(\"initial read:\", sample.INITIAL_READS)\n",
    "print(\"initial (pre-DS) reads aligned to human genome: {} ({:.1f} %)\".format(\n",
    "      sample.INITIAL_READS_ALIGNED, (sample.INITIAL_READS_ALIGNED * 100 / sample.INITIAL_READS)))\n",
    "print(\"reads aligned to arabidopsis genome: {} ({:.1f} %)\".format(\n",
    "        sample.PF_READS_ALIGNED_AR, (sample.PF_READS_ALIGNED_AR * 100 / sample.INITIAL_READS)) )\n",
    "FLAG_2_NREADS_NEB_10ng_1 = 89977265# Filtered on SAM flag 2\n",
    "assert sample.LIBRARY == \"NEB-10ng-1\"\n",
    "print(\"reads aligned PROPER PAIR to arabidopsis: {} ({:.1f} %)\".format(\n",
    "        FLAG_2_NREADS_NEB_10ng_1, (FLAG_2_NREADS_NEB_10ng_1 * 100 /sample.INITIAL_READS)) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping quality technical definition\n",
    "\n",
    "$$\n",
    "MQ = -\\log_{10} \\left( 1.0 - \\frac{10^{-SUM\\_BASE\\_Q(best)}}{\\sum_i 10^{-SUM\\_BASE\\_Q(i)}} \\right)\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "The mapping quality definition is defined based on the sum of base qualities of **mismatching bases**, as compared to the reference.\n",
    "\n",
    "The fraction above compares the mismatching base quality at the best hit to the mismatching base qualities at the other possible hits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping quality description\n",
    "\n",
    "The mapping quality is defined as a Phred-scaled probability that the mapped position of the read is the single best location in the genome. The probability is computed based on the sum of Q-scores of bases in the read that do not match the reference. A high mapping quality signifies that the read is uniquely mapped and a good match to the reference. Most reads will have a mapping quality of 60, indicating this condition. A mapping quality of 20 indicates a probability of 99 % that the read is mapped uniquely and correctly. Multi-mapping reads will in general have mapping qualities close to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is the KAPA kit so bad at producing aligned reads?\n",
    "\n",
    "The KAPA kit scores poorly on alignment, with a low percentage aligned, both using MAPQ cut-off or not.\n",
    "\n",
    "KAPA has a very broad insert size distribution, almost not peaking. We can compare with Swift, which has such a distribution for 100 ng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\n",
    "    \"Kit\": [\"Kapa\", \"Kapa\", \"Swift\", \"Swift\"],\n",
    "    \"Conc\":[\"10ng\", \"100ng\",\"10ng\",  \"100ng\"],\n",
    "    \"Broad distribution\": [True, True, False, True],\n",
    "    \"Bad align frac\": [True, False, False, True],\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The broad size distribution and bad alignment don't seem directly correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insert size (Picard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "colours = sns.color_palette()\n",
    "kits = df.Kit.cat.categories\n",
    "KIT_COL = dict(zip(kits, colours))\n",
    "print(KIT_COL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple plot with read count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mean_insert_sizes = []\n",
    "for conc in [\"10ng\", \"100ng\"]:\n",
    "    legends = set()\n",
    "    plt.figure()\n",
    "    plt.xlim(0, 900)\n",
    "    plt.title(conc)\n",
    "    for kit, group in df[df.Conc==conc].groupby((\"Kit\"), as_index=False):\n",
    "        colour = KIT_COL[kit]\n",
    "        means = []\n",
    "        for ix, (xs, ys) in group[['insert_size', 'All_Reads.fr_count']].iterrows():\n",
    "            if not kit in legends:\n",
    "                plt.plot(xs, ys, color=colour, label=kit)\n",
    "                legends.add(kit)\n",
    "            else:\n",
    "                plt.plot(xs, ys, color=colour)\n",
    "            # Mean\n",
    "            values = np.array(xs)\n",
    "            weights = np.array(ys)\n",
    "            mean = sum(weights*values) / sum(weights)\n",
    "            means.append(mean)\n",
    "        mean_insert_sizes.append((conc, kit, sum(means) / len(means)))\n",
    "            \n",
    "    plt.legend()\n",
    "    # Saving the next plot instead\n",
    "    #plt.savefig(\"plots/insert_size_{}.pdf\".format(conc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot with range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for conc in [\"10ng\", \"100ng\"]:\n",
    "    legends = set()\n",
    "    plt.figure()\n",
    "    plt.xlim(30, 900)\n",
    "    plt.title(conc)\n",
    "    for kit, group in df[df.Conc==conc].groupby((\"Kit\"), as_index=False):\n",
    "        colour = KIT_COL[kit]\n",
    "        means = []\n",
    "        for ix, (xs, ys) in group[['insert_size', 'All_Reads.fr_count']].iterrows():\n",
    "            x_sel = xs[30:]\n",
    "            y_sel = np.array(ys[30:])\n",
    "            y_sel = y_sel / 1e6 # y_sel.sum()\n",
    "            if not kit in legends:\n",
    "                plt.plot(x_sel, y_sel, color=colour, label=kit)\n",
    "                legends.add(kit)\n",
    "            else:\n",
    "                plt.plot(x_sel, y_sel, color=colour)\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Insert size\")\n",
    "    plt.ylabel(\"Frequency [millions of reads]\")\n",
    "    plt.savefig(\"plots/insert_size_{}.pdf\".format(conc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vvv in mean_insert_sizes:\n",
    "    print(*vvv, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nextera10_samples = df[(df.Conc==\"10ng\") & (df.Kit==\"Nextera\")]\n",
    "plt.figure()\n",
    "plt.xlim(100, 130)\n",
    "plt.ylim(0, 240000)\n",
    "plt.title(\"Nextera 10ng Zoom Length 100-130\")\n",
    "nextera10_samples[['insert_size', 'All_Reads.fr_count']].apply(\n",
    "            lambda x: plt.plot(*x.apply(lambda x: x[100:130]), '.'), \n",
    "    axis=1)\n",
    "plt.grid()\n",
    "plt.savefig(\"plots/nextera_insert_size_100.pdf\")\n",
    "plt.figure()\n",
    "plt.xlim(200, 230)\n",
    "plt.ylim(0, 240000)\n",
    "plt.title(\"Nextera 10ng Zoom Length 200-230\")\n",
    "nextera10_samples[['insert_size', 'All_Reads.fr_count']].apply(\n",
    "            lambda x: plt.plot(*x.apply(lambda x: x[200:230]), '.'), \n",
    "    axis=1)\n",
    "plt.grid()\n",
    "plt.savefig(\"plots/nextera_insert_size_200.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The period of the oscillations is approximately 10 bases. The magnitude is approximately 10000 reads, similar in both size ranges, less fraction of the reads at 200."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Short fragments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHORT_FRAG = 28\n",
    "for i, row in df_100.iterrows():\n",
    "    col = KIT_COL[row.Kit]\n",
    "    plt.plot(row['insert_size'], row['All_Reads.fr_count'], color=col)\n",
    "plt.plot(\n",
    "    [df['insert_size'][0][SHORT_FRAG], df['insert_size'][0][SHORT_FRAG]],\n",
    "    [0, max(df['All_Reads.fr_count'][0][0:SHORT_FRAG])],\n",
    "    'r'\n",
    "    )\n",
    "plt.xlim(0, 100)\n",
    "plt.xlabel(\"Insert size\")\n",
    "plt.ylabel(\"Number of reads\")\n",
    "plt.savefig(\"plots/short_insert_zoom_100ng.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['short_fragments_pct'] = df['All_Reads.fr_count'].map(lambda x: sum(x[0:SHORT_FRAG]) * 100.0 / sum(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.boxplot(data=df, x='Kit', y='short_fragments_pct', hue='Conc', linewidth=0.5, fliersize=3)\n",
    "plt.ylim(0, 4)\n",
    "plt.ylabel(\"Percentage of short fragments\")\n",
    "plt.xlabel(\"\")\n",
    "plt.savefig(\"plots/short_fragments_fraction.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The peak at short fragment lengths looks serious, but only accounts for max. 4 % of the reads. NEB has more such reads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert size related summary tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean(row):\n",
    "    values = row['insert_size']\n",
    "    freqs = row['All_Reads.fr_count']\n",
    "    mean = sum(v*f for v, f in zip(values, freqs)) / sum(freqs)\n",
    "\n",
    "def get_median(row):\n",
    "    values = row['insert_size']\n",
    "    freqs = row['All_Reads.fr_count']\n",
    "    midpoint = sum(freqs) / 2\n",
    "    acc = 0\n",
    "    loc = 0\n",
    "    while loc < len(freqs):\n",
    "        acc += freqs[loc]\n",
    "        if acc > midpoint:\n",
    "            return values[loc]\n",
    "        loc += 1\n",
    "    return values[-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby([\"Conc\", \"Kit\"])[['short_fragments_pct', 'MEDIAN_INSERT_SIZE', 'MEAN_INSERT_SIZE', 'MEDIAN_ABSOLUTE_DEVIATION', 'STANDARD_DEVIATION']].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO.maybe: reproducibility of insert size between replicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Duplicated reads\n",
    "\n",
    "This plot shows library duplication rate. The optical duplicates are also counted if the dataset mode is \"aa\" (analyse all)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['PERCENT_DUPLICATION_NON_OPTICAL'] = (\n",
    "        (df.READ_PAIR_DUPLICATES - df.READ_PAIR_OPTICAL_DUPLICATES)*2 + df.UNPAIRED_READ_DUPLICATES\n",
    "        ) * 100.0 / (\n",
    "        df.READ_PAIRS_EXAMINED*2 + df.UNPAIRED_READS_EXAMINED\n",
    "        )\n",
    "sns.boxplot(data=df, x='Kit', y='PERCENT_DUPLICATION_NON_OPTICAL', hue='Conc')\n",
    "#plt.title(\"Non-optical duplicates\")\n",
    "plt.ylabel(\"Percent duplicates\")\n",
    "plt.xlabel(\"\")\n",
    "plt.title(\"THIS PLOT IS WRONG, SEE subprojects/Undownsampled.ipynb\")\n",
    "plt.ylim(0, 6)\n",
    "###plt.savefig(\"plots/non_optical_duplicates.pdf\")\n",
    "### NOTE: Do not use this plot. The duplication rate is signficantly distorted by downsampling.\n",
    "###       Use notebook subprojects/Undownsampled.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: why does Quanta have so many dupes? 100ng has no PCR!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "quanta = df[(df.Kit == 'Quanta')]\n",
    "swift = df[(df.Kit == 'Swift2S')]\n",
    "for kdf in [quanta, swift]:\n",
    "    print(kdf.Kit.iloc[0])\n",
    "    dfs = []\n",
    "    for conc in ['10ng', '100ng']:\n",
    "        quanta_dup = pd.DataFrame({'Value': kdf[kdf.Conc == conc]['PERCENT_DUPLICATION_NON_OPTICAL']})\n",
    "        quanta_dup['DupType'] = 'Non-ExAmp'\n",
    "        quanta_dup['Conc'] = conc\n",
    "        dfs.append(quanta_dup)\n",
    "        quanta_dup2 = pd.DataFrame({'Value': kdf[kdf.Conc == conc]['PERCENT_DUPLICATION']*100})\n",
    "        quanta_dup2['DupType'] = 'Total'\n",
    "        quanta_dup2['Conc'] = conc\n",
    "        dfs.append(quanta_dup2)\n",
    "    cat = pd.concat(dfs)\n",
    "    sns.catplot(data=cat, x=\"DupType\", y=\"Value\", hue=\"Conc\")\n",
    "    plt.ylim(0, 12)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quanta[quanta.Conc=='100ng'][['LIBRARY', 'PERCENT_DUPLICATION_NON_OPTICAL']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question\n",
    "\n",
    "The 10 ng and 100 ng libraries were pooled and sequenced together for each kit. For Quanta, the 10ng libraries had a significantly greater yield from the sequencing. One possible reason is that the 100ng libraries have a broader size distribution, while 10ng have a peak at small size, and the smaller fragments clustered more effectively.\n",
    "\n",
    "The surpising observation is that the duplication called as \"optical\" by Picard is much greater in the 100ng libraries. The optical duplicates on HiSeq X are actually Ex-Amp-related duplicates, or well hoppers. The probability of well-hopping is related to the flow cell loading, which should obviously be the same for the entire lane. Either the longer fragments in Quanta-100ng have a higher chance to go hopping, or the duplication metric is distorted by the downsampling. See the notebook in subprojects/Undownsampled.ipynb to follow up this last possibility.\n",
    "\n",
    "#### Answer\n",
    "\n",
    "**The plot in Undownsampled notebook clearly confirms that the duplication rates are wrong in this notebook, and the error is produced by downsampling.** Use the plot in the other notebook!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genome coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference scale for coverage: HiSeqX single lane sequencing.\n",
    "\n",
    "The PF reads spec for HiSeqX is 2.6 to 3.0 billion per flow cell, so 2.6 B should also provide a 30X genome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"*All quantities below in Million reads*\")\n",
    "reads_for_30x = 2.6 * 1000 / 8\n",
    "print(\"PF reads per lane for 30X coverage, per end:\", reads_for_30x)\n",
    "expected_per_library_reads = (reads_for_30x * 4) / 8\n",
    "print(\"Expected reads per library, 8 libraries over 4 lanes:\", expected_per_library_reads)\n",
    "actual_reads = df.TOTAL_READS.mean() // (2*1000000) # Divide by two, as TOTAL_READS counts both PE ends\n",
    "print(\"Actual downsampled read count:\", actual_reads)\n",
    "expected_coverage_level = 30 * (actual_reads / reads_for_30x)\n",
    "print(\"\")\n",
    "print(\"Expected coverage level based on read count:\", expected_coverage_level)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_max_cov = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = df.loc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(test_sample['coverage'], kde=False,\n",
    "             hist_kws={\"weights\":test_sample['high_quality_coverage_count']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_sample['coverage'], test_sample['high_quality_coverage_count'])\n",
    "plt.xlim(0, plot_max_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for conc in [\"10ng\", \"100ng\"]:\n",
    "    legends = set()\n",
    "    plt.xlim(0, plot_max_cov)\n",
    "    plt.title(conc)\n",
    "    plt.ylabel(\"Number of genomic loci\")\n",
    "    plt.xlabel(\"Coverage (number of covering reads)\")\n",
    "\n",
    "    for kit, group in df[df.Conc==conc].groupby(\"Kit\", as_index=False):\n",
    "        colour = KIT_COL[kit]\n",
    "        for ix, (xs, ys) in group[['coverage', 'high_quality_coverage_count']].iterrows():\n",
    "            if not kit in legends:\n",
    "                plt.plot(xs, ys, color=colour, label=kit)\n",
    "            else:\n",
    "                plt.plot(xs, ys, color=colour)\n",
    "            legends.add(kit)\n",
    "    plt.legend()\n",
    "    plt.savefig(\"plots/coverage_{}.pdf\".format(conc))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of entries, reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ix, sample in df.iterrows():\n",
    "    hist_sum = sum(sample['high_quality_coverage_count'])\n",
    "    hist_weighted_sum = sum(c * n for c, n in zip(sample['coverage'], \n",
    "                                                  sample['high_quality_coverage_count']))\n",
    "    print(sample['LIBRARY'], \"\\t:\\t\", hist_sum, \"\\t / \", hist_weighted_sum)\n",
    "\n",
    "NUM_GENOMIC_LOC = hist_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cumulative coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cumulative_num_reads'] = df.high_quality_coverage_count.apply(np.cumsum)\n",
    "df['cumulative_frac_reads'] = df.cumulative_num_reads.apply(lambda x: x / NUM_GENOMIC_LOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.cumulative_num_reads.loc[3][-1] # Number of genomic loci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "legends = set()\n",
    "\n",
    "fig = plt.figure(figsize=(10, 20))\n",
    "axs = {\n",
    "    \"10ng\": fig.add_subplot(2, 1, 1),\n",
    "    \"100ng\": fig.add_subplot(2, 1, 2)\n",
    "}\n",
    "for conc, ax in axs.items():\n",
    "    ax.set_xlim(0, plot_max_cov)\n",
    "    ax.set_title(conc)\n",
    "    ax.set_ylabel(\"Fraction of genome\")\n",
    "    ax.set_xlabel(\"Coverage\")\n",
    "for (kit, conc), group in df.groupby((\"Kit\", \"Conc\"), as_index=False):\n",
    "    colour = KIT_COL[kit]\n",
    "    ax = axs[conc]\n",
    "    for ix, (xs, yys) in group[['coverage', 'cumulative_frac_reads']].iterrows():\n",
    "        ys = 1.0 - yys\n",
    "        if not (ax, kit) in legends:\n",
    "            ax.plot(xs, ys, color=colour, label=kit)\n",
    "        else:\n",
    "            ax.plot(xs, ys, color=colour)\n",
    "        legends.add((ax, kit))\n",
    "for ax in axs.values():\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "\n",
    "for kit, group in df[df.Conc=='100ng'].groupby(\"Kit\", as_index=False):\n",
    "    ys = 1.0 - np.mean(group['cumulative_frac_reads'])\n",
    "    xs = group['coverage'].iloc[0]\n",
    "    plt.plot(xs, ys, color=KIT_COL[kit], label=kit)\n",
    "\n",
    "plt.legend()\n",
    "plt.plot([expected_coverage_level, expected_coverage_level], [0.0,1.0], 'c--')\n",
    "plt.xlim(0, plot_max_cov)\n",
    "plt.ylim(0, 1)\n",
    "plt.title(\"averaged over replicates -- 100 ng\")\n",
    "plt.ylabel(\"Fraction of genome covered at or above\")\n",
    "plt.xlabel(\"Coverage\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = df_100.groupby('Kit')[\n",
    "    ['MEAN_COVERAGE','MEDIAN_COVERAGE', 'PCT_1X',\n",
    "     'PCT_10X', 'TOTAL_READS', 'INITIAL_READS']\n",
    "    ].mean()\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analysis was first done with a different reference, including ALTernative contigs. It's probably not a good idea to use these ALT contigs unless the whole pipeline takes them into account all the time.\n",
    "\n",
    "Kit,MEAN_COVERAGE,MEDIAN_COVERAGE,PCT_1X,PCT_10X,TOTAL_READS\n",
    "Kapa,4.84100525,5.0,0.90678325,0.044468,179998242.0\n",
    "NEB,4.42826475,4.25,0.9013639999999999,0.0293165,180011693.5\n",
    "Nextera,6.6390705,7.0,0.9147957499999999,0.1851845,179995698.5\n",
    "Quanta,4.7782167499999995,5.0,0.9069119999999999,0.041523500000000005,179997947.5\n",
    "Swift2S,4.76407625,4.75,0.9061827499999999,0.041947250000000005,179996364.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#table.to_csv(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['coverage'][0][0])\n",
    "print(df['high_quality_coverage_count'][0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note on interpretation of coverage histogram: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aligned reads more detailed, todo\n",
    "read_counts = []\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    library = {\n",
    "        'LIBRARY': row.LIBRARY,\n",
    "        'Kit': row.Kit,\n",
    "        'Conc': row.Conc\n",
    "    }\n",
    "    read_counts.append(dict(\n",
    "                        Reads=row.TOTAL_READS_PER_END,\n",
    "                        Type='Downsampled',\n",
    "                        **library))\n",
    "    aligned = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coverage calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
