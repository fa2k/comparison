{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import re\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis options\n",
    "\n",
    "We have to make the exact same analysis but with some different parameters. The following parameters change the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OBSOLETE OPTION -- Leave it set to \"aa\".\n",
    "# Which downsampling normalisation to use: This selects input data:\n",
    "# dd: Deduplicated downsampled data -- This is not particularly interesting\n",
    "# aa: Analyse all reads in data, downsample based on number of PF reads\n",
    "DS_MODE = \"aa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This option controls whether the Swift long-insert libraries are included in the analysis.\n",
    "# They were sequenced on the HiSeq 4000 with less coverage, and are thus not directly comparable.\n",
    "\n",
    "INCLUDE_SWIFT_LONG_LIBS = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alignment notes\n",
    "\n",
    "Alignment with bwa-mem.\n",
    "\n",
    "The minimum seed length is 19.\n",
    "\n",
    "Maximum gap length: 100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Picard data\n",
    "\n",
    "### Step 1 is to load all the metrics from the downsampled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_picard(path):\n",
    "    with open(path) as f:\n",
    "        mode = None\n",
    "        metrics_header = None\n",
    "        metrics_data = {}\n",
    "        histogram_series = []\n",
    "        histogram_data = []\n",
    "        for l in f:\n",
    "            line = l.rstrip(\"\\r\\n\")\n",
    "            if mode == 'metrics':\n",
    "                metrics_header = line.split()\n",
    "                mode = 'metrics2'\n",
    "            elif mode == 'metrics2':\n",
    "                if line != \"\":\n",
    "                    metrics_data = dict(zip(metrics_header, line.split()))\n",
    "                else:\n",
    "                    mode = None\n",
    "            elif mode == 'histogram':\n",
    "                histogram_series = line.split()\n",
    "                histogram_data = [list() for _ in histogram_series]\n",
    "                mode = 'histogram2'\n",
    "            elif mode == 'histogram2':\n",
    "                if line != \"\":\n",
    "                    for i, value in enumerate(line.split()):\n",
    "                        histogram_data[i].append(value)\n",
    "                else:\n",
    "                    mode = None\n",
    "            elif line.startswith(\"## METRICS CLASS\"):\n",
    "                mode = 'metrics'\n",
    "            elif line.startswith(\"## HISTOGRAM\"):\n",
    "                mode = 'histogram'\n",
    "        if histogram_series:\n",
    "            #metrics_data['histograms'] = dict(zip(histogram_series, histogram_data))\n",
    "            for name, data in zip(histogram_series, histogram_data):\n",
    "                metrics_data[name] = [float(x) for x in data]\n",
    "    return metrics_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = []\n",
    "for bam in glob(\"30_downsample/*-{}_DS_MD.AlignmentSummaryMetrics.txt\".format(DS_MODE)):\n",
    "    basepath = re.sub(r\"_MD\\.AlignmentSummaryMetrics\\.txt$\", \"\", bam)\n",
    "    data = {}\n",
    "    try:\n",
    "        for metrics in ['_MD.AlignmentSummaryMetrics', '_MD.InsertSizeMetrics',\n",
    "                        '.MarkDuplicatesMetrics', '_MD.WgsMetrics']:\n",
    "            new_data = load_picard(\"{}{}.txt\".format(basepath, metrics))\n",
    "            if any(k in data for k in new_data):\n",
    "                print(\"Duplicate key {} found in {}\".format(k, metrics))\n",
    "            data.update(new_data)\n",
    "    except FileNotFoundError as e:\n",
    "        print(\"File {} not found, skipping this sample.\".format(e.filename))\n",
    "        continue\n",
    "    samples.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Get original number of PF reads from before downsampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inital_read_counts = []\n",
    "for alignment_txt in glob(\"20_piccard/*.AlignmentSummaryMetrics.txt\"):\n",
    "    library = re.match(r\"([^/]+)\\.AlignmentSummaryMetrics.txt\", os.path.basename(alignment_txt)).group(1)\n",
    "    new_data = load_picard(alignment_txt)\n",
    "    inital_read_counts.append({'LIBRARY': library, 'INITIAL_READS': int(new_data['TOTAL_READS']),\n",
    "                                    'INITIAL_READS_PER_END': int(new_data['TOTAL_READS']) / 2,\n",
    "                                    'INITIAL_READS_ALIGNED': int(new_data['PF_READS_ALIGNED']),\n",
    "                                    'INITIAL_READS_ALIGNED_PER_END': int(new_data['PF_READS_ALIGNED']) / 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dedup_read_counts = []\n",
    "for rc_txt in glob(\"20_piccard_dd/*.readCount.txt\"):\n",
    "    library = re.match(r\"([^/]+)\\.readCount\\.txt\", os.path.basename(rc_txt)).group(1)\n",
    "    rc = int(open(rc_txt).read().strip())\n",
    "    dedup_read_counts.append({'LIBRARY': library, 'DEDUPLICATED_READS': rc, 'DEDUPLICATED_READS_PER_END': rc/2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Get arabidopsis metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arabidopsis_samples = []\n",
    "for asm in glob(\"arabidopsis/20_piccard/*.AlignmentSummaryMetrics.txt\"):\n",
    "    basepath = re.sub(r\"\\.AlignmentSummaryMetrics\\.txt$\", \"\", asm)\n",
    "    data = {}\n",
    "    for metrics in ['.AlignmentSummaryMetrics', '.InsertSizeMetrics',\n",
    "                    '.MarkDuplicatesMetrics', '.WgsMetrics']:\n",
    "        new_data = load_picard(\"{}{}.txt\".format(basepath, metrics))\n",
    "        if any(k in data for k in new_data):\n",
    "            print(\"Duplicate key {} found in {}\".format(k, metrics))\n",
    "        data.update(new_data)\n",
    "    arabidopsis_samples.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if arabidopsis_samples:\n",
    "    print(\"We have arabidopsis data\")\n",
    "else:\n",
    "    print(\"We don't have arabidopsis data, but will continue anyway\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main_df = pd.DataFrame(samples).reindex().apply(pd.to_numeric, axis=0, errors='ignore')\n",
    "in_reads_df = pd.DataFrame(inital_read_counts)\n",
    "dd_reads_df = pd.DataFrame(dedup_read_counts)\n",
    "arabidopsis_df = pd.DataFrame(arabidopsis_samples).apply(pd.to_numeric, axis=0, errors='ignore')\n",
    "\n",
    "\n",
    "# Without Arabidopsis data\n",
    "if arabidopsis_df.empty:\n",
    "    df = pd.merge( \n",
    "        pd.merge(\n",
    "            main_df, \n",
    "            in_reads_df,  \n",
    "            on='LIBRARY').reindex(), \n",
    "        dd_reads_df,  \n",
    "        on='LIBRARY')\n",
    "else: # With Arabidopsis\n",
    "    df = pd.merge( \n",
    "        pd.merge( \n",
    "            pd.merge( \n",
    "                main_df, \n",
    "                arabidopsis_df, \n",
    "                on='LIBRARY', suffixes=('', '_AR')).reindex(), \n",
    "            in_reads_df,  \n",
    "            on='LIBRARY').reindex(), \n",
    "        dd_reads_df,  \n",
    "        on='LIBRARY')\n",
    "    df.dtypes[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['Kit'] = pd.Categorical(df.LIBRARY.str.split(\"-\").str.get(0))\n",
    "df['Conc'] = pd.Categorical(df.LIBRARY.str.split(\"-\").str.get(1), categories=['10ng', '100ng'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally remove `Swiftlong`, and fix the order of kits and concentrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not INCLUDE_SWIFT_LONG_LIBS:\n",
    "    df = df.loc[df.Kit != \"Swiftlong\"]\n",
    "    df.Kit = df.Kit.cat.set_categories(['Kapa', 'NEB', 'Nextera', 'Quanta', 'Swift2S'])\n",
    "else:\n",
    "    df.Kit = df.Kit.cat.set_categories(['Kapa', 'NEB', 'Nextera', 'Quanta', 'Swift2S', 'Swiftlong'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_100 = df.loc[df.Conc == \"100ng\"]\n",
    "df.head(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of entries: \", len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def savefig(name, main=True):\n",
    "    for fmt in ['png', 'pdf']:\n",
    "        if main:\n",
    "            if INCLUDE_SWIFT_LONG_LIBS:\n",
    "                plt.savefig('plots-main-withLong/{}.{}'.format(name, fmt))\n",
    "            else:\n",
    "                plt.savefig('plots-main/{}.{}'.format(name, fmt))\n",
    "        else:\n",
    "            if INCLUDE_SWIFT_LONG_LIBS:\n",
    "                plt.savefig('plots-suppl-withLong/{}.{}'.format(name, fmt))\n",
    "            else:\n",
    "                plt.savefig('plots-suppl/{}.{}'.format(name, fmt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of reads\n",
    "\n",
    "Number of PE reads (reads per end):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DOWNSAMPLING_TARGET_READS = 90e6\n",
    "\n",
    "df['TOTAL_READS_PER_END'] = df['TOTAL_READS'] / 2\n",
    "num_reads_downsampled = df.TOTAL_READS_PER_END.mean()\n",
    "num_reads_initial = df.INITIAL_READS_PER_END.mean()\n",
    "num_reads_deduplicated = df.DEDUPLICATED_READS_PER_END.mean()\n",
    "print(\"One lane of HiSeqX:       {:9d} (optimistic)\".format(400000000))\n",
    "print(\"Initial reads (avg):      {:9d}\".format(int(num_reads_initial)))\n",
    "print(\"Deduplicated reads (avg): {:9d}\".format(int(num_reads_deduplicated)))\n",
    "print(\"Downsampled reads:        {:9d}\".format(int(num_reads_downsampled)))\n",
    "print(\"Target downsampled reads  {:9d}\".format(int(DOWNSAMPLING_TARGET_READS)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment set-up:\n",
    "\n",
    "One pool for each kit. There are 8 libraries in each pool. The pool is sequenced on four lanes. Each library gets a half lane's worth of reads on average.\n",
    "\n",
    "The mean read count per library is around half of the expected 200M, because of less than optimal yield, and inevitable differences between samples in the pool.\n",
    "\n",
    "The \"downsampled\" in the table above depends on the constant set at the top of this notebook. It is either based on the lowest Initial reads (aa) or the lowest Deduplicated reads (dd)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(DS_MODE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downsampling is based on the following read counts: aa=INITIAL, dd=DEDUPLICATED.\n",
    "\n",
    "The point with the fewest reads represents the value used for downsampling (after rounding down)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=df, x='Kit', y='INITIAL_READS_PER_END', hue='Conc')\n",
    "plt.ylabel('Number of reads per replicate')\n",
    "plt.xlabel(\"\")\n",
    "savefig(\"total_number_of_reads\")\n",
    "plt.show()\n",
    "sns.boxplot(data=df, x='Kit', y='DEDUPLICATED_READS_PER_END', hue='Conc')\n",
    "plt.title('Deduplicated reads')\n",
    "plt.show()\n",
    "sns.boxplot(data=df, x='Kit', y='TOTAL_READS_PER_END', hue='Conc')\n",
    "plt.title('Downsampled reads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[(df.Kit=='Swiftlong') & (df.Conc == '100ng')].PF_READS/2)\n",
    "print((df[(df.Kit=='Swiftlong') & (df.Conc == '100ng')].PF_READS/2).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aligned reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([col for col in list(df) if 'ALIGNED' in col.upper()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot of any aligned reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=df, x='Kit', y='PCT_PF_READS_ALIGNED', hue='Conc')\n",
    "plt.title('Reads aligned to human genome')\n",
    "plt.ylabel(\"Fraction of reads\")\n",
    "plt.xlabel(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots for MAPQ > 20 reads\n",
    "\n",
    "Requiring MAPQ > 20 removes a large percentage of reads. We found a lot of reads that map to both references."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['PCT_HQ_PF_READS_ALIGNED'] = df.PF_HQ_ALIGNED_READS / df.TOTAL_READS\n",
    "sns.boxplot(data=df, x='Kit', y='PCT_HQ_PF_READS_ALIGNED', hue='Conc')\n",
    "plt.title('Reads aligned to human genome (MAPQ>=20)')\n",
    "plt.ylabel(\"Fraction of reads\")\n",
    "plt.xlabel(\"\")\n",
    "savefig(\"aligned_reads_human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Low aligned rate\")\n",
    "df[df.PCT_HQ_PF_READS_ALIGNED < 0.88][[\"LIBRARY\", \"PCT_HQ_PF_READS_ALIGNED\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct the aligned pct. All reads in the AR dataset are aligned.\n",
    "if not arabidopsis_df.empty:\n",
    "    df['PCT_HQ_PF_READS_ALIGNED_AR'] = df.PF_HQ_ALIGNED_READS_AR / df.INITIAL_READS\n",
    "    sns.boxplot(data=df, x='Kit', y='PCT_HQ_PF_READS_ALIGNED_AR', hue='Conc')\n",
    "    plt.title('Reads aligned to Arabidopsis Lyrata genome (MAPQ >= 20)')\n",
    "    plt.ylabel(\"Fraction of reads\")\n",
    "    plt.xlabel(\"\")\n",
    "    savefig(\"aligned_reads_arabidopsis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show arabidopsis in same plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not arabidopsis_df.empty:\n",
    "    grid = sns.boxplot(data=df, x='Kit', y='PCT_HQ_PF_READS_ALIGNED', hue='Conc', linewidth=0.5, fliersize=3)\n",
    "    grid.legend().remove()\n",
    "    sns.boxplot(data=df, x='Kit', y='PCT_HQ_PF_READS_ALIGNED_AR', hue='Conc', linewidth=0.5, fliersize=3)\n",
    "    plt.ylim(0,1)\n",
    "    savefig(\"aligned_reads_both\", False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative way to do combined plot here -- not used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not arabidopsis_df.empty:\n",
    "    #df['PCT_HQ_PF_READS_ALIGNED_AR'] = df.PF_HQ_ALIGNED_READS_AR / df.INITIAL_READS\n",
    "    #df['PCT_HQ_PF_READS_ALIGNED'] = df.PF_HQ_ALIGNED_READS / df.TOTAL_READS\n",
    "    hdf = pd.DataFrame(df)\n",
    "    hdf['Value'] = hdf.PCT_HQ_PF_READS_ALIGNED\n",
    "    hdf['Species'] = \"Human\"\n",
    "    adf = pd.DataFrame(df)\n",
    "    adf['Value'] = adf.PCT_HQ_PF_READS_ALIGNED_AR\n",
    "    adf['Species'] = \"Arabidopsis\"\n",
    "    combined = pd.concat([adf, hdf])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show details for one sample. This is to determine why there is so many reads aligned to both arabidopsis and human at the same time. This was never completely solved, but we see that the problem goes away when requiring a moderate mapping quality (20)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not arabidopsis_df.empty:\n",
    "    sample = df[df['LIBRARY'] == 'NEB-10ng-1'].iloc[0]\n",
    "    print(\"sample:\", sample.LIBRARY)\n",
    "    print(\"initial read:\", sample.INITIAL_READS)\n",
    "    print(\"initial (pre-DS) reads aligned to human genome: {}Â ({:.1f} %)\".format(\n",
    "          sample.INITIAL_READS_ALIGNED, (sample.INITIAL_READS_ALIGNED * 100 / sample.INITIAL_READS)))\n",
    "    print(\"reads aligned to arabidopsis genome: {} ({:.1f} %)\".format(\n",
    "            sample.PF_READS_ALIGNED_AR, (sample.PF_READS_ALIGNED_AR * 100 / sample.INITIAL_READS)) )\n",
    "    FLAG_2_NREADS_NEB_10ng_1 = 89977265# Filtered on SAM flag 2\n",
    "    assert sample.LIBRARY == \"NEB-10ng-1\"\n",
    "    print(\"reads aligned PROPER PAIR to arabidopsis: {} ({:.1f} %)\".format(\n",
    "            FLAG_2_NREADS_NEB_10ng_1, (FLAG_2_NREADS_NEB_10ng_1 * 100 /sample.INITIAL_READS)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping quality technical definition\n",
    "\n",
    "$$\n",
    "MQ = -\\log_{10} \\left( 1.0 - \\frac{10^{-SUM\\_BASE\\_Q(best)}}{\\sum_i 10^{-SUM\\_BASE\\_Q(i)}} \\right)\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "The mapping quality definition is defined based on the sum of base qualities of **mismatching bases**, as compared to the reference.\n",
    "\n",
    "The fraction above compares the mismatching base quality at the best hit to the mismatching base qualities at the other possible hits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping quality description\n",
    "\n",
    "The mapping quality is defined as a Phred-scaled probability that the mapped position of the read is the single best location in the genome. The probability is computed based on the sum of Q-scores of bases in the read that do not match the reference. A high mapping quality signifies that the read is uniquely mapped and a good match to the reference. Most reads will have a mapping quality of 60, indicating this condition. A mapping quality of 20 indicates a probability of 99 % that the read is mapped uniquely and correctly. Multi-mapping reads will in general have mapping qualities close to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is the KAPA kit so bad at producing aligned reads?\n",
    "\n",
    "The KAPA kit scores poorly on alignment, with a low percentage aligned, both using MAPQ cut-off or not.\n",
    "\n",
    "KAPA has a very broad insert size distribution, almost not peaking. We can compare with Swift, which has such a distribution for 100 ng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\n",
    "    \"Kit\": [\"Kapa\", \"Kapa\", \"Swift\", \"Swift\"],\n",
    "    \"Conc\":[\"10ng\", \"100ng\",\"10ng\",  \"100ng\"],\n",
    "    \"Broad distribution\": [True, True, False, True],\n",
    "    \"Bad align frac\": [True, False, False, True],\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The broad size distribution and bad alignment don't seem directly correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insert size (Picard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "colours = sns.color_palette()\n",
    "kits = df.Kit.cat.categories\n",
    "KIT_COL = dict(zip(kits, colours))\n",
    "print(KIT_COL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple plot with read count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_insert_sizes = []\n",
    "for conc in [\"10ng\", \"100ng\"]:\n",
    "    legends = set()\n",
    "    plt.figure()\n",
    "    plt.xlim(0, 900)\n",
    "    plt.title(conc)\n",
    "    for kit, group in df[df.Conc==conc].groupby((\"Kit\"), as_index=False):\n",
    "        colour = KIT_COL[kit]\n",
    "        means = []\n",
    "        for ix, (xs, ys) in group[['insert_size', 'All_Reads.fr_count']].iterrows():\n",
    "            if not kit in legends:\n",
    "                plt.plot(xs, ys, color=colour, label=kit)\n",
    "                legends.add(kit)\n",
    "            else:\n",
    "                plt.plot(xs, ys, color=colour)\n",
    "            # Mean\n",
    "            values = np.array(xs)\n",
    "            weights = np.array(ys)\n",
    "            mean = sum(weights*values) / sum(weights)\n",
    "            means.append(mean)\n",
    "        mean_insert_sizes.append((conc, kit, sum(means) / len(means)))\n",
    "            \n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot with range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for conc in [\"10ng\", \"100ng\"]:\n",
    "    legends = set()\n",
    "    plt.figure()\n",
    "    plt.xlim(30, 900)\n",
    "    plt.title(conc)\n",
    "    for kit, group in df[df.Conc==conc].groupby((\"Kit\"), as_index=False):\n",
    "        colour = KIT_COL[kit]\n",
    "        means = []\n",
    "        for ix, (xs, ys) in group[['insert_size', 'All_Reads.fr_count']].iterrows():\n",
    "            x_sel = xs[30:]\n",
    "            y_sel = np.array(ys[30:])\n",
    "            y_sel = y_sel / 1e6 # y_sel.sum()\n",
    "            if not kit in legends:\n",
    "                plt.plot(x_sel, y_sel, color=colour, label=kit)\n",
    "                legends.add(kit)\n",
    "            else:\n",
    "                plt.plot(x_sel, y_sel, color=colour)\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Insert size\")\n",
    "    plt.ylabel(\"Frequency [millions of reads]\")\n",
    "    savefig(\"insert_size_{}\".format(conc), False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vvv in mean_insert_sizes:\n",
    "    print(*vvv, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nextera10_samples = df[(df.Conc==\"10ng\") & (df.Kit==\"Nextera\")]\n",
    "plt.figure()\n",
    "plt.xlim(100, 130)\n",
    "plt.ylim(0, 240000)\n",
    "plt.title(\"Nextera 10ng Zoom Length 100-130\")\n",
    "nextera10_samples[['insert_size', 'All_Reads.fr_count']].apply(\n",
    "            lambda x: plt.plot(*x.apply(lambda x: x[100:130]), '.'), \n",
    "    axis=1)\n",
    "plt.grid()\n",
    "savefig(\"nextera_insert_size_100\", False)\n",
    "plt.figure()\n",
    "plt.xlim(200, 230)\n",
    "plt.ylim(0, 240000)\n",
    "plt.title(\"Nextera 10ng Zoom Length 200-230\")\n",
    "nextera10_samples[['insert_size', 'All_Reads.fr_count']].apply(\n",
    "            lambda x: plt.plot(*x.apply(lambda x: x[200:230]), '.'), \n",
    "    axis=1)\n",
    "plt.grid()\n",
    "savefig(\"nextera_insert_size_200\", False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The period of the oscillations is approximately 10 bases. The magnitude is approximately 10000 reads, similar in both size ranges, less fraction of the reads at 200."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Short fragments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHORT_FRAG = 28\n",
    "for i, row in df_100.iterrows():\n",
    "    col = KIT_COL[row.Kit]\n",
    "    plt.plot(row['insert_size'], row['All_Reads.fr_count'], color=col)\n",
    "plt.plot(\n",
    "    [df['insert_size'][0][SHORT_FRAG], df['insert_size'][0][SHORT_FRAG]],\n",
    "    [0, max(df['All_Reads.fr_count'][0][0:SHORT_FRAG])],\n",
    "    'r'\n",
    "    )\n",
    "plt.xlim(0, 100)\n",
    "plt.xlabel(\"Insert size\")\n",
    "plt.ylabel(\"Number of reads\")\n",
    "savefig(\"short_insert_zoom_100ng\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['short_fragments_pct'] = df['All_Reads.fr_count'].map(lambda x: sum(x[0:SHORT_FRAG]) * 100.0 / sum(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=df, x='Kit', y='short_fragments_pct', hue='Conc', linewidth=0.5, fliersize=3)\n",
    "plt.ylim(0, 4)\n",
    "plt.ylabel(\"Percentage of short fragments\")\n",
    "plt.xlabel(\"\")\n",
    "savefig(\"short_fragments_fraction\", False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The peak at short fragment lengths looks serious, but only accounts for max. 4 % of the reads. NEB has more such reads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert size related summary tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean(row):\n",
    "    values = row['insert_size']\n",
    "    freqs = row['All_Reads.fr_count']\n",
    "    mean = sum(v*f for v, f in zip(values, freqs)) / sum(freqs)\n",
    "\n",
    "def get_median(row):\n",
    "    values = row['insert_size']\n",
    "    freqs = row['All_Reads.fr_count']\n",
    "    midpoint = sum(freqs) / 2\n",
    "    acc = 0\n",
    "    loc = 0\n",
    "    while loc < len(freqs):\n",
    "        acc += freqs[loc]\n",
    "        if acc > midpoint:\n",
    "            return values[loc]\n",
    "        loc += 1\n",
    "    return values[-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_size = df.groupby([\"Conc\", \"Kit\"])[['short_fragments_pct', 'MEDIAN_INSERT_SIZE', 'MEAN_INSERT_SIZE', 'MEDIAN_ABSOLUTE_DEVIATION', 'STANDARD_DEVIATION']].mean()\n",
    "insert_size.to_csv(\"result-tables/insert-size.csv\")\n",
    "insert_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO.maybe: reproducibility of insert size between replicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Duplicated reads\n",
    "\n",
    "The downsampling significantly distorted the duplication rates.\n",
    "\n",
    "The old code and plots for duplicates were junk and have been deleted.\n",
    "\n",
    "See the notebook in subprojects/Undownsampled.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genome coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference scale for coverage: HiSeqX single lane sequencing.\n",
    "\n",
    "The PF reads spec for HiSeqX is 2.6 to 3.0 billion per flow cell, so 2.6 B should also provide a 30X genome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"*All quantities below in Million reads*\")\n",
    "reads_for_30x = 2.6 * 1000 / 8\n",
    "print(\"PF reads per lane for 30X coverage, per end:\", reads_for_30x)\n",
    "expected_per_library_reads = (reads_for_30x * 4) / 8\n",
    "print(\"Expected reads per library, 8 libraries over 4 lanes:\", expected_per_library_reads)\n",
    "actual_reads = df.TOTAL_READS.mean() // (2*1000000) # Divide by two, as TOTAL_READS counts both PE ends\n",
    "print(\"Actual downsampled read count:\", actual_reads)\n",
    "expected_coverage_level = 30 * (actual_reads / reads_for_30x)\n",
    "print(\"\")\n",
    "print(\"Expected coverage level based on read count:\", expected_coverage_level)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_max_cov = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = df.loc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(test_sample['coverage'], kde=False,\n",
    "             hist_kws={\"weights\":test_sample['high_quality_coverage_count']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_sample['coverage'], test_sample['high_quality_coverage_count'])\n",
    "plt.xlim(0, plot_max_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for conc in [\"10ng\", \"100ng\"]:\n",
    "    legends = set()\n",
    "    plt.xlim(0, plot_max_cov)\n",
    "    plt.title(conc)\n",
    "    plt.ylabel(\"Number of genomic loci\")\n",
    "    plt.xlabel(\"Coverage (number of covering reads)\")\n",
    "\n",
    "    for kit, group in df[df.Conc==conc].groupby(\"Kit\", as_index=False):\n",
    "        colour = KIT_COL[kit]\n",
    "        for ix, (xs, ys) in group[['coverage', 'high_quality_coverage_count']].iterrows():\n",
    "            if not kit in legends:\n",
    "                plt.plot(xs, ys, color=colour, label=kit)\n",
    "            else:\n",
    "                plt.plot(xs, ys, color=colour)\n",
    "            legends.add(kit)\n",
    "    plt.legend()\n",
    "    savefig(\"/coverage_{}\".format(conc), False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of entries, reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ix, sample in df.iterrows():\n",
    "    hist_sum = sum(sample['high_quality_coverage_count'])\n",
    "    hist_weighted_sum = sum(c * n for c, n in zip(sample['coverage'], \n",
    "                                                  sample['high_quality_coverage_count']))\n",
    "    print(sample['LIBRARY'], \"\\t:\\t\", hist_sum, \"\\t / \", hist_weighted_sum)\n",
    "\n",
    "NUM_GENOMIC_LOC = hist_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cumulative coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cumulative_num_reads'] = df.high_quality_coverage_count.apply(np.cumsum)\n",
    "df['cumulative_frac_reads'] = df.cumulative_num_reads.apply(lambda x: x / NUM_GENOMIC_LOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.cumulative_num_reads.loc[3][-1] # Number of genomic loci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "legends = set()\n",
    "\n",
    "fig = plt.figure(figsize=(10, 20))\n",
    "axs = {\n",
    "    \"10ng\": fig.add_subplot(2, 1, 1),\n",
    "    \"100ng\": fig.add_subplot(2, 1, 2)\n",
    "}\n",
    "for conc, ax in axs.items():\n",
    "    ax.set_xlim(0, plot_max_cov)\n",
    "    ax.set_title(conc)\n",
    "    ax.set_ylabel(\"Fraction of genome\")\n",
    "    ax.set_xlabel(\"Coverage\")\n",
    "for (kit, conc), group in df.groupby((\"Kit\", \"Conc\"), as_index=False):\n",
    "    colour = KIT_COL[kit]\n",
    "    ax = axs[conc]\n",
    "    for ix, (xs, yys) in group[['coverage', 'cumulative_frac_reads']].iterrows():\n",
    "        ys = 1.0 - yys\n",
    "        if not (ax, kit) in legends:\n",
    "            ax.plot(xs, ys, color=colour, label=kit)\n",
    "        else:\n",
    "            ax.plot(xs, ys, color=colour)\n",
    "        legends.add((ax, kit))\n",
    "for ax in axs.values():\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "\n",
    "for kit, group in df[df.Conc=='100ng'].groupby(\"Kit\", as_index=False):\n",
    "    ys = 1.0 - np.mean(group['cumulative_frac_reads'])\n",
    "    xs = group['coverage'].iloc[0]\n",
    "    plt.plot(xs, ys, color=KIT_COL[kit], label=kit)\n",
    "\n",
    "plt.legend()\n",
    "plt.plot([expected_coverage_level, expected_coverage_level], [0.0,1.0], 'c--')\n",
    "plt.xlim(0, plot_max_cov)\n",
    "plt.ylim(0, 1)\n",
    "plt.title(\"averaged over replicates -- 100 ng\")\n",
    "plt.ylabel(\"Fraction of genome covered at or above\")\n",
    "plt.xlabel(\"Coverage\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean coverage results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_coverage_summary = df.groupby(['Conc','Kit'], as_index=False)[\n",
    "    ['MEAN_COVERAGE','MEDIAN_COVERAGE', 'PCT_1X',\n",
    "     'PCT_10X', 'TOTAL_READS', 'INITIAL_READS']\n",
    "    ].mean()\n",
    "mean_coverage_summary.to_csv(\"result-tables/mean-coverage.csv\")\n",
    "mean_coverage_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=df, x=\"Kit\", y=\"MEAN_COVERAGE\", hue=\"Conc\")\n",
    "plt.ylim(0, 8)\n",
    "plt.ylabel('Mean coverage per {:.0f}M read pairs'.format(DOWNSAMPLING_TARGET_READS/1e6))\n",
    "plt.xlabel(\"\")\n",
    "savefig(\"mean_coverage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analysis was first done with a different reference, including ALTernative contigs. It's probably not a good idea to use these ALT contigs unless the whole pipeline takes them into account all the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_old_ref = \"\"\"Kit,MEAN_COVERAGE,MEDIAN_COVERAGE,PCT_1X,PCT_10X,TOTAL_READS\n",
    "Kapa,4.84100525,5.0,0.90678325,0.044468,179998242.0\n",
    "NEB,4.42826475,4.25,0.9013639999999999,0.0293165,180011693.5\n",
    "Nextera,6.6390705,7.0,0.9147957499999999,0.1851845,179995698.5\n",
    "Quanta,4.7782167499999995,5.0,0.9069119999999999,0.041523500000000005,179997947.5\n",
    "Swift2S,4.76407625,4.75,0.9061827499999999,0.041947250000000005,179996364.0\"\"\"\n",
    "pd.DataFrame((c.split(',') for c in result_old_ref.splitlines()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#table.to_csv(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['coverage'][0][0])\n",
    "print(df['high_quality_coverage_count'][0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note on interpretation of coverage histogram: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aligned reads more detailed, todo\n",
    "read_counts = []\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    library = {\n",
    "        'LIBRARY': row.LIBRARY,\n",
    "        'Kit': row.Kit,\n",
    "        'Conc': row.Conc\n",
    "    }\n",
    "    read_counts.append(dict(\n",
    "                        Reads=row.TOTAL_READS_PER_END,\n",
    "                        Type='Downsampled',\n",
    "                        **library))\n",
    "    aligned = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coverage calculation detail: Counting of overlapping reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
